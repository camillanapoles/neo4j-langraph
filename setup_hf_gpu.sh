#!/bin/bash
# Script para configurar modelos HuggingFace com GPU

set -e

echo "ðŸ¤— Configurando modelos HuggingFace com GPU..."
echo "=============================================="
echo ""

# Verificar se hÃ¡ GPU NVIDIA disponÃ­vel
if command -v nvidia-smi &> /dev/null; then
    echo "ðŸŽ® GPU NVIDIA detectada!"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
    echo ""
else
    echo "âš ï¸ GPU NVIDIA nÃ£o detectada, usando CPU"
    echo ""
fi

echo "ðŸ“¦ Instalando dependÃªncias de GPU..."
echo ""

# Instalar dependÃªncias GPU
uv pip install torch --index-url https://download.pytorch.org/whl/cu121
uv pip install transformers sentence-transformers accelerate

echo ""
echo "â¬‡ï¸ Baixando modelos HuggingFace..."
echo ""

# Script Python para baixar modelos
cat > /tmp/download_models.py << 'EOF'
import os
from sentence_transformers import SentenceTransformer
import torch

print("Verificando disponibilidade de GPU...")
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Dispositivo: {device}")
if device == "cuda":
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"MemÃ³ria: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
print()

print("â¬‡ï¸ Baixando modelo de embeddings...")
model_name = os.getenv("HF_EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
print(f"Modelo: {model_name}")

model = SentenceTransformer(model_name)
print(f"âœ… Modelo baixado e carregado em {device}!")
print(f"DimensÃµes de embeddings: {model.get_sentence_embedding_dimension()}")
EOF

# Executar download
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2 .venv/bin/python /tmp/download_models.py

echo ""
echo "âœ… ConfiguraÃ§Ã£o completa!"
echo ""
echo "ðŸ“ PrÃ³ximos passos:"
echo "1. Copiar configuraÃ§Ã£o GPU:"
echo "   cp .env.gpu .env"
echo ""
echo "2. Editar .env para usar EMBEDDING_BACKEND=huggingface"
echo ""
echo "3. Testar no sistema:"
echo "   .venv/bin/python test_system.py"
echo ""
