#!/usr/bin/env python3
"""
Script para baixar modelo LLM no LocalAI
Modelo: Llama-3.2-3B-Instruct (Q4_K_M - quantizaÃ§Ã£o 4-bit, ~1.8GB)
"""

import os
import requests
import time

print("ğŸ“¥ BAIXANDO MODELO LLM NO LOCALAI")
print("=" * 60)
print()

# URL do modelo (GGUF quantizado)
model_url = "https://huggingface.co/MaziyarPanahi/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct.Q4_K_M.gguf"
model_name = "llama3.2:3b"
localai_url = "http://localhost:30808/v1"

print(f"ğŸ¤– Modelo: {model_name}")
print(f"ğŸ“¦ URL: {model_url}")
print(f"ğŸ“Š Tamanho: ~1.8GB (Q4_K_M quantizaÃ§Ã£o)")
print()

# Verificar se LocalAI estÃ¡ rodando
print("ğŸ”Œ Verificando LocalAI...")
try:
    response = requests.get(f"{localai_url}/models", timeout=5)
    if response.status_code == 200:
        print("  âœ… LocalAI rodando")
    else:
        print(f"  âŒ LocalAI erro: {response.status_code}")
        exit(1)
except Exception as e:
    print(f"  âŒ Erro ao conectar: {e}")
    print("  ğŸ’¡ Inicie o LocalAI:")
    print("     kubectl get pods -n neo4j-langraph")
    exit(1)

print()

# Baixar modelo no LocalAI
print("ğŸš€ Baixando modelo no LocalAI...")
print("  â³ Isso pode levar alguns minutos...")
print()

try:
    response = requests.post(
        f"{localai_url}/models/{model_name}",
        json={"url": model_url},
        timeout=300  # 5 minutos
    )

    if response.status_code == 200:
        print("âœ… Modelo baixado com sucesso!")
        print()
        print("ğŸ“Š Modelo carregado:")
        print(f"  â€¢ Nome: {model_name}")
        print(f"  â€¢ Tipo: GGUF (quantizado)")
        print(f"  â€¢ Tamanho: ~1.8GB")
        print(f"  â€¢ VRAM: ~2GB")
        print()
    else:
        print(f"âŒ Erro ao baixar: {response.status_code}")
        print(f"  Mensagem: {response.text}")
        exit(1)

except Exception as e:
    print(f"âŒ Erro: {e}")
    exit(1)

# Verificar modelo disponÃ­vel
print("ğŸ” Verificando se modelo estÃ¡ disponÃ­vel...")
try:
    response = requests.get(f"{localai_url}/models", timeout=5)
    models = response.json().get("data", [])

    print(f"ğŸ“Š Modelos disponÃ­veis: {len(models)}")
    for model in models:
        print(f"  â€¢ {model.get('id')}")
    print()

    # Verificar se nosso modelo estÃ¡ na lista
    model_ids = [m.get('id') for m in models]
    if model_name in model_ids:
        print(f"âœ… Modelo {model_name} estÃ¡ disponÃ­vel!")
    else:
        print(f"âš ï¸  Modelo {model_name} nÃ£o estÃ¡ na lista")
        print("  ğŸ’¡ Pode estar carregando...")
        print("  Aguarde alguns minutos e teste novamente")

except Exception as e:
    print(f"âŒ Erro ao verificar: {e}")

print()
print("=" * 60)
print("ğŸ‰ MODELO LLM PRONTO PARA USO!")
print("=" * 60)
print()
print("ğŸ® Modelo estÃ¡ pronto para ser usado via LiteLLM:")
print("  â€¢ Endpoint: http://localhost:30808/v1")
print("  â€¢ Model ID: localai/llama3.2:3b")
print()
