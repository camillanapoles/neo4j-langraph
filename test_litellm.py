#!/usr/bin/env python3
"""
Script de teste do LiteLLM
Roteia entre Gemini Flash 2.5 (primÃ¡rio) e LocalAI (secundÃ¡rio)
"""

import os
from dotenv import load_dotenv
from openai import OpenAI

# Carregar variÃ¡veis de ambiente
load_dotenv()

print("=" * 60)
print("ğŸ¤– LITELLM - TESTE DE ROTEAMENTO")
print("=" * 60)
print()

# Configurar cliente LiteLLM (usa formato OpenAI)
print("ğŸ“Š ConfiguraÃ§Ã£o:")
print("  â€¢ Modelo PrimÃ¡rio:   gemini/gemini-2.0-flash-exp")
print("  â€¢ Modelo SecundÃ¡rio: localai/llama3.2:3b")
print("  â€¢ EstratÃ©gia:        usage-based-routing")
print("  â€¢ Fallback:          localai/llama3.2:3b")
print()

# Criar cliente (aponta para LiteLLM Proxy)
print("ğŸ”Œ Conectando ao LiteLLM Proxy...")
try:
    client = OpenAI(
        api_key="sk-litellm-master-key",  # LiteLLM master key
        base_url="http://localhost:4000/v1"
    )
    print("âœ… Conectado ao LiteLLM Proxy!")
    print()

except Exception as e:
    print(f"âŒ Erro ao conectar: {e}")
    print()
    print("ğŸ’¡ SoluÃ§Ã£o: Inicie o LiteLLM Proxy:")
    print("   bash start_litellm.sh")
    exit(1)

# Listar modelos disponÃ­veis
print("ğŸ“Š Modelos disponÃ­veis:")
try:
    models = client.models.list()
    for model in models.data:
        print(f"  â€¢ {model.id}")
    print()
except Exception as e:
    print(f"âš ï¸  NÃ£o foi possÃ­vel listar modelos: {e}")
    print()

# Testar roteamento
print("ğŸ§ª Testando roteamento...")
print()

try:
    # Fazer uma query
    print("ğŸ“ Enviando query...")
    response = client.chat.completions.create(
        model="gemini-flash",  # Modelo do grupo (LiteLLM roteia automaticamente)
        messages=[
            {"role": "user", "content": "OlÃ¡! Responda em 1 frase: Qual seu nome?"}
        ],
        temperature=0.7,
        max_tokens=50
    )

    print("âœ… Query enviada com sucesso!")
    print()
    print(f"ğŸ“ Resposta: {response.choices[0].message.content}")
    print()
    print(f"ğŸ“Š Modelo usado: {response.model}")
    print()

except Exception as e:
    print(f"âŒ Erro na query: {e}")
    print()
    print("ğŸ’¡ PossÃ­veis causas:")
    print("   1. Gemini Flash: Cota excedida (429)")
    print("   2. LocalAI: Modelo nÃ£o baixado")
    print("   3. LiteLLM Proxy: NÃ£o estÃ¡ rodando")
    print()
    print("ğŸ’¡ SoluÃ§Ãµes:")
    print("   1. Baixar modelo LLM: python3 download_llm_model.py")
    print("   2. Iniciar LiteLLM: bash start_litellm.sh")
    exit(1)

# Resumo final
print("=" * 60)
print("ğŸ‰ LITELLM ROTEAMENTO TESTADO!")
print("=" * 60)
print()
print("âœ… Sistema de roteamento funcionando!")
print()
print("ğŸ“Š Como funciona:")
print("  1. Query Ã© enviada para LiteLLM Proxy")
print("  2. LiteLLM roteia para Gemini Flash (primÃ¡rio)")
print("  3. Se Gemini falhar (cota 429), roteia para LocalAI")
print("  4. Fallback automÃ¡tico e transparente")
print()
