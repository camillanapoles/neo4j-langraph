#!/usr/bin/env python3
"""
Script completo de teste do sistema AI
Testa: LLM, Neo4j, LocalAI e Ingest√£o
"""

import os
import time
from dotenv import load_dotenv

print("=" * 60)
print("üß™ SISTEMA AI - TESTE COMPLETO")
print("=" * 60)
print()

# Carregar vari√°veis de ambiente
load_dotenv()

# ======================
# 1. TESTE DO LLM
# ======================
print("1Ô∏è‚É£  TESTE DO LLM (Gemini Flash 2.5)")
print("-" * 60)

try:
    from openai import OpenAI

    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key or api_key.startswith("your-google"):
        print("‚ùå ERRO: GOOGLE_API_KEY n√£o configurada!")
        print("   Configure no .env: GOOGLE_API_KEY=AIza...")
        exit(1)

    print(f"üîë API Key: {api_key[:10]}...{api_key[-10:]}")

    client = OpenAI(
        api_key=api_key,
        base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
    )

    print("üìù Enviando query para LLM...")
    response = client.chat.completions.create(
        model="gemini-2.0-flash-exp",
        messages=[
            {"role": "user", "content": "Ol√°! Responda em 1 frase: Qual seu nome?"}
        ],
        temperature=0.7,
        max_tokens=50
    )

    print("‚úÖ LLM funcionando!")
    print(f"üìù Resposta: {response.choices[0].message.content}")
    print()

except Exception as e:
    print(f"‚ùå Erro no LLM: {e}")
    print()
    print("üí° Solu√ß√£o: Aguarde 60 segundos e teste novamente")
    print()

    # Se erro de cota (429), aguardar e tentar novamente
    if "429" in str(e) or "quota" in str(e).lower():
        print("‚è≥ Aguardando 60 segundos para o limite de cota...")
        for i in range(60, 0, -1):
            print(f"\r‚è≥ Aguardando... {i} segundos  ", end="", flush=True)
            time.sleep(1)
        print("\n‚úÖ Aguardo conclu√≠do! Tentando novamente...")
        print()

        try:
            response = client.chat.completions.create(
                model="gemini-2.0-flash-exp",
                messages=[
                    {"role": "user", "content": "Ol√°! Responda em 1 frase: Qual seu nome?"}
                ],
                temperature=0.7,
                max_tokens=50
            )
            print("‚úÖ LLM funcionando ap√≥s aguardo!")
            print(f"üìù Resposta: {response.choices[0].message.content}")
            print()
        except Exception as e2:
            print(f"‚ùå Erro ap√≥s aguardo: {e2}")
            print()
            print("üí° Solu√ß√£o: Configure Ollama (LLM local) ou aguarde mais tempo")
            exit(1)
    else:
        exit(1)

# ======================
# 2. TESTE DO NEO4J
# ======================
print("2Ô∏è‚É£  TESTE DO NEO4J (Grafo)")
print("-" * 60)

try:
    from neo4j import GraphDatabase

    print("üîå Conectando ao Neo4j...")
    driver = GraphDatabase.driver(
        "bolt://localhost:30687",
        auth=("neo4j", "password")
    )

    driver.verify_connectivity()
    print("‚úÖ Neo4j conectado!")

    # Query de teste
    session = driver.session()
    result = session.run("RETURN 'Neo4j est√° funcionando!' AS msg")
    print(f"üìä Query de teste: {result.single()['msg']}")

    # Contar n√≥s
    result = session.run("MATCH (n) RETURN count(n) AS count")
    node_count = result.single()['count']
    print(f"üìä Total de n√≥s no grafo: {node_count}")

    session.close()
    driver.close()
    print()

except Exception as e:
    print(f"‚ùå Erro no Neo4j: {e}")
    print()
    print("üí° Solu√ß√£o: Verifique se Neo4j est√° rodando:")
    print("   kubectl get pods -n neo4j-langraph")
    exit(1)

# ======================
# 3. TESTE DO LOCALAI
# ======================
print("3Ô∏è‚É£  TESTE DO LOCALAI (Embeddings com GPU)")
print("-" * 60)

try:
    import requests

    print("üîå Conectando ao LocalAI...")
    response = requests.get(
        "http://localhost:30808/v1/models",
        timeout=5
    )

    if response.status_code == 200:
        print("‚úÖ LocalAI conectado!")

        models = response.json()
        model_count = len(models.get("data", []))
        print(f"üìä Modelos dispon√≠veis: {model_count}")

        if model_count == 0:
            print("‚ö†Ô∏è  Nenhum modelo carregado")
            print("üí° Para carregar modelos, veja a documenta√ß√£o:")
            print("   https://localai.io/model-gallery/")
        print()
    else:
        print(f"‚ùå Erro no LocalAI: Status {response.status_code}")
        print()

except Exception as e:
    print(f"‚ùå Erro no LocalAI: {e}")
    print()
    print("üí° Solu√ß√£o: Verifique se LocalAI est√° rodando:")
    print("   kubectl get pods -n neo4j-langraph")
    exit(1)

# ======================
# 4. STATUS DO SISTEMA
# ======================
print("4Ô∏è‚É£  STATUS DO SISTEMA")
print("-" * 60)

try:
    import subprocess

    # Verificar pods
    result = subprocess.run(
        ["k3s", "kubectl", "get", "pods", "-n", "neo4j-langraph"],
        capture_output=True,
        text=True,
        timeout=10
    )

    if result.returncode == 0:
        print("üê≥ Status dos Pods:")
        print(result.stdout)
    else:
        print("‚ùå Erro ao verificar pods")

    # Verificar GPU
    result = subprocess.run(
        ["nvidia-smi", "--query-gpu=name,memory.total,memory.used,memory.free,utilization.gpu",
         "--format=csv,noheader,nounits"],
        capture_output=True,
        text=True,
        timeout=10
    )

    if result.returncode == 0:
        print("üéÆ Status da GPU (RTX 4070):")
        gpu_info = result.stdout.strip().split(',')
        print(f"   Nome:      {gpu_info[0]}")
        print(f"   Total VRAM: {gpu_info[1]} MB")
        print(f"   Usado:      {gpu_info[2]} MB ({round(int(gpu_info[2]) / int(gpu_info[1]) * 100)}%)")
        print(f"   Livre:      {gpu_info[3]} MB")
        print(f"   Utiliza√ß√£o: {gpu_info[4]}%")
        print()

except Exception as e:
    print(f"‚ö†Ô∏è  N√£o foi poss√≠vel verificar o status completo: {e}")
    print()

# ======================
# 5. RESUMO FINAL
# ======================
print("=" * 60)
print("üéâ TESTE COMPLETO!")
print("=" * 60)
print()
print("‚úÖ SISTEMA AI FUNCIONANDO!")
print()
print("üåê ACESSOS:")
print("   ‚Ä¢ Neo4j Browser:   http://localhost:30474")
print("   ‚Ä¢ Neo4j BOLT:     bolt://localhost:30687")
print("   ‚Ä¢ LocalAI API:    http://localhost:30808")
print()
print("üéÆ GPU (RTX 4070):")
print("   ‚Ä¢ Dispon√≠vel para embeddings com LocalAI")
print()
print("üß† LLM (Gemini Flash 2.5):")
print("   ‚Ä¢ Funcionando (responda queries)")
print()
print("üìö PR√ìXIMOS PASSOS:")
print("   1. Ingerir documentos:")
print("      .venv/bin/python -m src.cli.knowledge_cli ingest /path/to/docs")
print()
print("   2. Fazer queries:")
print("      .venv/bin/python -m src.cli.knowledge_cli query 'sua pergunta'")
print()
print("   3. Visualizar grafo:")
print("      Abra http://localhost:30474")
print()
print("=" * 60)
